[2024-12-23 08:38:25,473 main.py:229 INFO] Detected system ID: KnownSystem.RTX4090x2
/home/cmuser/.local/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
/home/cmuser/.local/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
[2024-12-23 08:38:27,072 generate_conf_files.py:107 INFO] Generated measurements/ entries for RTX4090x2_TRT/stable-diffusion-xl/Server
[2024-12-23 08:38:27,072 __init__.py:46 INFO] Running command: python3 -m code.stable-diffusion-xl.tensorrt.harness --logfile_outdir="/cm-mount/home/arjun/gh_action_results/valid_results/RTX4090x2-nvidia_original-gpu-tensorrt-vdefault-default_config/stable-diffusion-xl/server/performance/run_1" --logfile_prefix="mlperf_log_" --performance_sample_count=5000 --test_mode="PerformanceOnly" --gpu_batch_size=2 --mlperf_conf_path="/home/cmuser/CM/repos/local/cache/5860c00d55d14786/inference/mlperf.conf" --tensor_path="build/preprocessed_data/coco2014-tokenized-sdxl/5k_dataset_final/" --use_graphs=true --user_conf_path="/home/cmuser/CM/repos/mlcommons@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/154dc0ba8b834b358b5beea980395141.conf" --gpu_inference_streams=1 --gpu_copy_streams=1 --sdxl_batcher_time_limit=2 --gpu_engines="./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIP-Server-gpu-b2-fp16.custom_k_99_MaxP.plan,./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIPWithProj-Server-gpu-b2-fp16.custom_k_99_MaxP.plan,./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-UNetXL-Server-gpu-b2-int8.custom_k_99_MaxP.plan,./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-VAE-Server-gpu-b2-fp32.custom_k_99_MaxP.plan" --scenario Server --model stable-diffusion-xl
[2024-12-23 08:38:27,072 __init__.py:53 INFO] Overriding Environment
/home/cmuser/.local/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
/home/cmuser/.local/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
[2024-12-23 08:38:28,960 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIP-Server-gpu-b2-fp16.custom_k_99_MaxP.plan.
[2024-12-23 08:38:29,105 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIPWithProj-Server-gpu-b2-fp16.custom_k_99_MaxP.plan.
[2024-12-23 08:38:29,799 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-UNetXL-Server-gpu-b2-int8.custom_k_99_MaxP.plan.
[2024-12-23 08:38:31,233 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-VAE-Server-gpu-b2-fp32.custom_k_99_MaxP.plan.
[2024-12-23 08:38:32,430 backend.py:96 INFO] Enabling cuda graphs for unet
[2024-12-23 08:38:32,606 backend.py:154 INFO] captured graph for BS=1
[2024-12-23 08:38:32,861 backend.py:154 INFO] captured graph for BS=2
[2024-12-23 08:38:33,006 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIP-Server-gpu-b2-fp16.custom_k_99_MaxP.plan.
[2024-12-23 08:38:33,138 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIPWithProj-Server-gpu-b2-fp16.custom_k_99_MaxP.plan.
[2024-12-23 08:38:33,823 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-UNetXL-Server-gpu-b2-int8.custom_k_99_MaxP.plan.
[2024-12-23 08:38:35,248 backend.py:71 INFO] Loading TensorRT engine: ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-VAE-Server-gpu-b2-fp32.custom_k_99_MaxP.plan.
[2024-12-23 08:38:36,411 backend.py:96 INFO] Enabling cuda graphs for unet
[2024-12-23 08:38:36,550 backend.py:154 INFO] captured graph for BS=1
[2024-12-23 08:38:36,800 backend.py:154 INFO] captured graph for BS=2
[2024-12-23 08:38:36,801 harness.py:207 INFO] Start Warm Up!
[2024-12-23 08:38:48,161 harness.py:209 INFO] Warm Up Done!
[2024-12-23 08:38:48,161 harness.py:211 INFO] Start Test!
[2024-12-23 08:48:53,766 backend.py:801 INFO] [Server] Received 535 total samples
[2024-12-23 08:48:53,767 backend.py:809 INFO] [Device 0] Reported 268 samples
[2024-12-23 08:48:53,768 backend.py:809 INFO] [Device 1] Reported 267 samples
[2024-12-23 08:48:53,768 harness.py:214 INFO] Test Done!
[2024-12-23 08:48:53,768 harness.py:216 INFO] Destroying SUT...
[2024-12-23 08:48:53,768 harness.py:219 INFO] Destroying QSL...
benchmark : Benchmark.SDXL
buffer_manager_thread_count : 0
data_dir : /home/cmuser/CM/repos/local/cache/4db00c74da1e44c8/data
gpu_batch_size : 2
gpu_copy_streams : 1
gpu_inference_streams : 1
input_dtype : int32
input_format : linear
log_dir : /home/cmuser/CM/repos/local/cache/94a57f78972843c6/repo/closed/NVIDIA/build/logs/2024.12.23-08.38.23
mlperf_conf_path : /home/cmuser/CM/repos/local/cache/5860c00d55d14786/inference/mlperf.conf
model_path : /home/cmuser/CM/repos/local/cache/4db00c74da1e44c8/models/SDXL/
precision : int8
preprocessed_data_dir : /home/cmuser/CM/repos/local/cache/4db00c74da1e44c8/preprocessed_data
scenario : Scenario.Server
sdxl_batcher_time_limit : 2
server_target_qps : 0
server_target_qps_adj_factor : 0.0
system : SystemConfiguration(host_cpu_conf=CPUConfiguration(layout={CPU(name='Intel(R) Xeon(R) w7-2495X', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, core_count=24, threads_per_core=2): 1}), host_mem_conf=MemoryConfiguration(host_memory_capacity=Memory(quantity=197.334532, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=197334532000), comparison_tolerance=0.05), accelerator_conf=AcceleratorConfiguration(layout=defaultdict(<class 'int'>, {GPU(name='NVIDIA GeForce RTX 4090', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=23.98828125, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25757220864), max_power_limit=450.0, pci_id='0x268410DE', compute_sm=89): 1, GPU(name='NVIDIA GeForce RTX 4090', accelerator_type=<AcceleratorType.Discrete: AliasedName(name='Discrete', aliases=(), patterns=())>, vram=Memory(quantity=23.98828125, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25757220864), max_power_limit=500.0, pci_id='0x268410DE', compute_sm=89): 1})), numa_conf=NUMAConfiguration(numa_nodes={}, num_numa_nodes=1), system_id='RTX4090x2')
tensor_path : build/preprocessed_data/coco2014-tokenized-sdxl/5k_dataset_final/
test_mode : PerformanceOnly
use_graphs : True
user_conf_path : /home/cmuser/CM/repos/mlcommons@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/154dc0ba8b834b358b5beea980395141.conf
system_id : RTX4090x2
config_name : RTX4090x2_stable-diffusion-xl_Server
workload_setting : WorkloadSetting(HarnessType.Custom, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 1
config_ver : custom_k_99_MaxP
accuracy_level : 99%
inference_server : custom
skip_file_checks : False
power_limit : None
cpu_freq : None
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIP-Server-gpu-b2-fp16.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIPWithProj-Server-gpu-b2-fp16.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-UNetXL-Server-gpu-b2-int8.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-VAE-Server-gpu-b2-fp32.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIP-Server-gpu-b2-fp16.custom_k_99_MaxP.plan
[W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-CLIPWithProj-Server-gpu-b2-fp16.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-UNetXL-Server-gpu-b2-int8.custom_k_99_MaxP.plan
[I] Loading bytes from ./build/engines/RTX4090x2/stable-diffusion-xl/Server/stable-diffusion-xl-VAE-Server-gpu-b2-fp32.custom_k_99_MaxP.plan
[2024-12-23 08:48:54,290 run_harness.py:166 INFO] Result: result_scheduled_samples_per_sec: 0.888638, Result is VALID
 
======================== Result summaries: ========================

